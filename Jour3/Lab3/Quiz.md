Question 1 : Différence entre ERROR, WARNING et INFO dans les logs Hop
ERROR indique une erreur critique qui empêche le pipeline de fonctionner correctement, comme une connexion à la base de données échouée ou une transformation qui plante. WARNING signale un problème potentiel qui n'empêche pas l'exécution mais mérite attention, comme un taux de rejet anormalement élevé ou une performance dégradée. INFO fournit des informations contextuelles sur le déroulement normal du pipeline, comme le nombre de lignes traitées ou le début d'une étape, utiles pour le suivi opérationnel sans indiquer de problème.

Question 2 : Pourquoi un pipeline sans plantage peut être défectueux
Un pipeline peut s'exécuter sans erreur technique mais produire des résultats incorrects. Il peut traiter zéro ligne sans lever d'erreur si le fichier source est vide ou introuvable. Il peut rejeter silencieusement 90% des données sans alerte si les règles de validation sont trop strictes. Il peut écrire des données corrompues ou incomplètes dans la destination sans que le système ne le détecte. La logique métier peut être fausse, comme un calcul erroné qui produit des montants incorrects mais techniquement valides. Un pipeline "vert" en monitoring ne garantit donc pas la qualité des résultats.

Question 3 : Comment identifier la transformation la plus lente
Les logs Hop affichent le temps d'exécution de chaque transformation en millisecondes ou secondes. Il faut chercher les lignes indiquant "Finished processing" ou "Step finished" avec le temps écoulé. On peut comparer les timestamps de début et fin de chaque transformation. L'interface graphique de Hop affiche aussi des métriques de performance en temps réel pendant l'exécution. Les transformations qui traitent peu de lignes par seconde ou qui accumulent des lignes en attente sont les goulots d'étranglement à optimiser.

Question 4 : Chute soudaine du nombre de lignes sans erreur explicite
Plusieurs causes silencieuses peuvent expliquer cette situation. Un filtre trop restrictif ou une règle de validation modifiée rejette massivement des données valides. La source de données contient moins d'enregistrements qu'habituellement, par exemple un fichier partiel ou une extraction incomplète. Une jointure avec une table de référence vide ou obsolète élimine toutes les lignes. Un problème de réseau ou de permissions fait que le système lit un fichier vide sans erreur. Une condition logique mal configurée filtre par erreur la majorité des données. Il faut vérifier les compteurs à chaque étape pour localiser où les lignes disparaissent.

Question 5 : Pourquoi les logs sont indispensables en traitement nocturne automatisé
Personne n'est présent pour surveiller l'exécution en temps réel pendant la nuit. Les logs permettent d'investiguer post-mortem en cas d'échec pour comprendre ce qui s'est passé exactement. Ils facilitent le diagnostic rapide le matin sans devoir relancer le traitement. Ils permettent de vérifier que tous les traitements se sont bien exécutés dans l'ordre et dans les temps. Ils servent de preuve pour les audits et la traçabilité réglementaire en montrant quand et comment les données ont été traitées. Sans logs, un échec nocturne peut rester invisible jusqu'à ce que les utilisateurs signalent des données manquantes.

Question 6 : Différence entre log système et audit métier
Un log système enregistre les événements techniques du pipeline comme les démarrages, arrêts, erreurs de connexion, et performances des transformations, destinés aux data engineers pour le debugging. Un audit métier trace les opérations business significatives comme qui a modifié quelles données, quand, et pourquoi, les décisions de validation prises, et les montants calculés, destinés aux équipes métier et aux auditeurs. Le log système est volatile et peut être purgé régulièrement, tandis que l'audit métier doit être conservé longtemps pour conformité. Le log système est technique et détaillé, l'audit métier est synthétique et orienté business.

Question 7 : Pourquoi simuler volontairement une erreur est une bonne pratique
La simulation permet de vérifier que les mécanismes de gestion d'erreur fonctionnent correctement avant qu'une vraie erreur ne survienne en production. Elle aide à comprendre comment le système réagit et quels logs sont générés pour faciliter le debugging futur. Elle permet de tester les alertes et notifications configurées pour s'assurer qu'elles se déclenchent au bon moment. Elle forme les équipes à diagnostiquer et résoudre les problèmes dans un environnement contrôlé sans pression. Elle valide que les flux de rejet et les mécanismes de rollback fonctionnent comme prévu. Mieux vaut découvrir les failles en test qu'en production avec des données réelles.

Question 8 : Comment détecter un goulot d'étranglement dans un pipeline
Il faut analyser les logs pour identifier les transformations avec le temps d'exécution le plus long. Observer le débit de lignes traitées par seconde pour chaque étape permet de repérer celles qui traitent anormalement lentement. Les transformations qui accumulent des lignes en attente sans les consommer rapidement sont des goulots. Comparer le temps total d'exécution avec la somme des temps individuels révèle les attentes et blocages. Les pics d'utilisation CPU ou mémoire corrélés avec certaines transformations indiquent des opérations coûteuses. Les jointures, aggregations et écritures en base de données sont souvent les candidates principales à optimiser par indexation, parallélisation ou batch processing.